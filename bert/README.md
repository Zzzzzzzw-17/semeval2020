# Contextualised Word Representations with BERT

### Code

Under `code/bert` we implement lexical semantic change detection using BERT representations. It involves two stages:

1. `collect.py`: collection of contextualised representations
2. `distance.py` or `relatedness.py`: quantification of semantic change

First, we collect all contextualised representations for the target lemmas (`collect.py`). Then, we measure the degree of change 
undergone by a lemma in terms of the geometric properties of its representations. For the latter step, we can either 
(1) calculate the mean pairwise distance between sets of representations collected across two time intervals 
(`distance.py`), or (2) calculate the diachronic difference in mean relatedness (`relatedness.py`).

The script `run-bert.sh` will run, for now, only the collection phase on the [trial data](#markdown-header-trial-data). 
For this, assuming you are working on a UNIX-based system, first make the script executable with

	chmod 755 run-bert.sh

Then execute

	bash -e run-bert.sh

The script will unzip the data, iterate over corpora of each language, collect contextualised representations, 
store them under `matrices/`.
<!--
and write the results for the trial targets under `results/`. It will also produce answer files for task 1 and 2 in the 
required submission format from the results and store them under `results/`. It does this in the following way: FD and 
CNT+CI+CD predict change values for the target words. These values provide the ranking for task 2. Then, target words 
are assigned into two classes depending on whether their predicted change values exceed a specified threshold or not. 
If the script throws errors, you might need to install Python dependencies: `pip3 install -r requirements.txt`.
-->

### Trial Data <a name="markdown-header-trial-data"></a>

We provide trial data in `trial_data_public.zip`. For each language, it contains:

- trial target words for which predictions can be submitted in the practice phase (`targets/`)
- the true classification of the trial target words for task 1 in the practice phase, i.e., the file against which
submissions will be scored in the practice phase (`truth/task1/`)
- the true ranking of the trial target words for task 2 in the practice phase (`truth/task2/`)
- a sample submission for the trial target words in the above-specified format (`answer.zip/`)
- two trial corpora from which you may predict change scores for the trial target words (`corpora/`)

__Important__: The scores in `truth/task1/` and `truth/task2/` are not meaningful as they were randomly assigned.

You can start by uploading the zipped answer folder to the system to check the submission and evaluation format. 
Find more information on the submission format on [the shared task website](https://languagechange.org/semeval/).

#### Trial Corpora ####

The trial corpora under `corpora/` are gzipped samples from the corpora that will be used in the evaluation phase. 
For each language two time-specific corpora are provided. Participants are required to predict the lexical semantic 
change of the target words between these two corpora. Each line contains one sentence and has the form

	lemma1 lemma2 lemma3...

Sentences have been randomly shuffled. The corpora have the same format as the ones which will be used in the evaluation
phase. Find more information about the corpora on [the shared task website](https://languagechange.org/semeval/).


References <a name="references"></a>
--------

Dominik Schlechtweg, Anna HÃ¤tty, Marco del Tredici, and Sabine Schulte im Walde. 2019. [A Wind of Change: Detecting and 
Evaluating Lexical Semantic Change across Times and Domains](https://www.aclweb.org/anthology/papers/P/P19/P19-1072/). In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Florence, Italy. ACL.